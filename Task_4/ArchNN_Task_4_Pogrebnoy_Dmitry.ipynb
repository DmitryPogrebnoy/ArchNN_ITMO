{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O4Vm-vIiLwk"
      },
      "source": [
        "# Walker2D\n",
        "\n",
        "You aim in this task is to train the agent to win in Walker2D game with Actor-Critic, Advantage Actor Critic (A2C), Trust-region Policy Optimization (TRPO) or Proximal Policy Optimization (PPO).\n",
        "To solve the task feel free to transform the state and reward from the environment.\n",
        "\n",
        "**Scoring**: Calculating the average reward for 50 episodes. You goal is to gain more than 1000 points.\n",
        "\n",
        "**Submission format**: send you notebook and trained model in **zipped** folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9gjbf9Gw6pD",
        "outputId": "5c178671-81ed-4ae3-b14b-35004dc31e9a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSqcgvyqXP2f",
        "outputId": "5d5ca567-efb7-4af7-d1b8-a877de1bf9a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rXDVWmoEgt55"
      },
      "outputs": [],
      "source": [
        "! pip install PyBullet >> None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGGXGwMIWO9F",
        "outputId": "2e9e866d-292c-4d09-f5eb-7d5acc280822"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                       Version\n",
            "----------------------------- ----------------------\n",
            "absl-py                       1.3.0\n",
            "aeppl                         0.0.33\n",
            "aesara                        2.7.9\n",
            "aiohttp                       3.8.3\n",
            "aiosignal                     1.3.1\n",
            "alabaster                     0.7.12\n",
            "albumentations                1.2.1\n",
            "altair                        4.2.0\n",
            "appdirs                       1.4.4\n",
            "arviz                         0.12.1\n",
            "astor                         0.8.1\n",
            "astropy                       4.3.1\n",
            "astunparse                    1.6.3\n",
            "async-timeout                 4.0.2\n",
            "atari-py                      0.2.9\n",
            "atomicwrites                  1.4.1\n",
            "attrs                         22.1.0\n",
            "audioread                     3.0.0\n",
            "autograd                      1.5\n",
            "Babel                         2.11.0\n",
            "backcall                      0.2.0\n",
            "beautifulsoup4                4.6.3\n",
            "bleach                        5.0.1\n",
            "blis                          0.7.9\n",
            "bokeh                         2.3.3\n",
            "branca                        0.6.0\n",
            "bs4                           0.0.1\n",
            "CacheControl                  0.12.11\n",
            "cachetools                    5.2.0\n",
            "catalogue                     2.0.8\n",
            "certifi                       2022.9.24\n",
            "cffi                          1.15.1\n",
            "cftime                        1.6.2\n",
            "chardet                       3.0.4\n",
            "charset-normalizer            2.1.1\n",
            "click                         7.1.2\n",
            "clikit                        0.6.2\n",
            "cloudpickle                   1.5.0\n",
            "cmake                         3.22.6\n",
            "cmdstanpy                     1.0.8\n",
            "colorcet                      3.0.1\n",
            "colorlover                    0.3.0\n",
            "community                     1.0.0b1\n",
            "confection                    0.0.3\n",
            "cons                          0.4.5\n",
            "contextlib2                   0.5.5\n",
            "convertdate                   2.4.0\n",
            "crashtest                     0.3.1\n",
            "crcmod                        1.7\n",
            "cufflinks                     0.17.3\n",
            "cvxopt                        1.3.0\n",
            "cvxpy                         1.2.2\n",
            "cycler                        0.11.0\n",
            "cymem                         2.0.7\n",
            "Cython                        0.29.32\n",
            "daft                          0.0.4\n",
            "dask                          2022.2.1\n",
            "datascience                   0.17.5\n",
            "db-dtypes                     1.0.4\n",
            "debugpy                       1.0.0\n",
            "decorator                     4.4.2\n",
            "defusedxml                    0.7.1\n",
            "descartes                     1.1.0\n",
            "dill                          0.3.6\n",
            "distributed                   2022.2.1\n",
            "dlib                          19.24.0\n",
            "dm-tree                       0.1.7\n",
            "dnspython                     2.2.1\n",
            "docutils                      0.17.1\n",
            "dopamine-rl                   1.0.5\n",
            "earthengine-api               0.1.334\n",
            "easydict                      1.10\n",
            "ecos                          2.0.10\n",
            "editdistance                  0.5.3\n",
            "en-core-web-sm                3.4.1\n",
            "entrypoints                   0.4\n",
            "ephem                         4.1.3\n",
            "et-xmlfile                    1.1.0\n",
            "etils                         0.9.0\n",
            "etuples                       0.3.8\n",
            "fa2                           0.3.5\n",
            "fastai                        2.7.10\n",
            "fastcore                      1.5.27\n",
            "fastdownload                  0.0.7\n",
            "fastdtw                       0.3.4\n",
            "fastjsonschema                2.16.2\n",
            "fastprogress                  1.0.3\n",
            "fastrlock                     0.8.1\n",
            "feather-format                0.4.1\n",
            "filelock                      3.8.0\n",
            "firebase-admin                5.3.0\n",
            "fix-yahoo-finance             0.0.22\n",
            "Flask                         1.1.4\n",
            "flatbuffers                   1.12\n",
            "folium                        0.12.1.post1\n",
            "frozenlist                    1.3.3\n",
            "fsspec                        2022.11.0\n",
            "future                        0.16.0\n",
            "gast                          0.4.0\n",
            "GDAL                          2.2.2\n",
            "gdown                         4.4.0\n",
            "gensim                        3.6.0\n",
            "geographiclib                 1.52\n",
            "geopy                         1.17.0\n",
            "gin-config                    0.5.0\n",
            "glob2                         0.7\n",
            "google                        2.0.3\n",
            "google-api-core               2.8.2\n",
            "google-api-python-client      1.12.11\n",
            "google-auth                   2.15.0\n",
            "google-auth-httplib2          0.0.4\n",
            "google-auth-oauthlib          0.4.6\n",
            "google-cloud-bigquery         3.3.6\n",
            "google-cloud-bigquery-storage 2.16.2\n",
            "google-cloud-core             2.3.2\n",
            "google-cloud-datastore        2.9.0\n",
            "google-cloud-firestore        2.7.2\n",
            "google-cloud-language         2.6.1\n",
            "google-cloud-storage          2.5.0\n",
            "google-cloud-translate        3.8.4\n",
            "google-colab                  1.0.0\n",
            "google-crc32c                 1.5.0\n",
            "google-pasta                  0.2.0\n",
            "google-resumable-media        2.4.0\n",
            "googleapis-common-protos      1.57.0\n",
            "googledrivedownloader         0.4\n",
            "graphviz                      0.10.1\n",
            "greenlet                      2.0.1\n",
            "grpcio                        1.51.1\n",
            "grpcio-status                 1.48.2\n",
            "gspread                       3.4.2\n",
            "gspread-dataframe             3.0.8\n",
            "gym                           0.25.2\n",
            "gym-notices                   0.0.8\n",
            "h5py                          3.1.0\n",
            "HeapDict                      1.0.1\n",
            "hijri-converter               2.2.4\n",
            "holidays                      0.17.2\n",
            "holoviews                     1.14.9\n",
            "html5lib                      1.0.1\n",
            "httpimport                    0.5.18\n",
            "httplib2                      0.17.4\n",
            "httpstan                      4.6.1\n",
            "humanize                      0.5.1\n",
            "hyperopt                      0.1.2\n",
            "idna                          2.10\n",
            "imageio                       2.9.0\n",
            "imagesize                     1.4.1\n",
            "imbalanced-learn              0.8.1\n",
            "imblearn                      0.0\n",
            "imgaug                        0.4.0\n",
            "importlib-metadata            4.13.0\n",
            "importlib-resources           5.10.0\n",
            "imutils                       0.5.4\n",
            "inflect                       2.1.0\n",
            "intel-openmp                  2022.2.1\n",
            "intervaltree                  2.1.0\n",
            "ipykernel                     5.3.4\n",
            "ipython                       7.9.0\n",
            "ipython-genutils              0.2.0\n",
            "ipython-sql                   0.3.9\n",
            "ipywidgets                    7.7.1\n",
            "itsdangerous                  1.1.0\n",
            "jax                           0.3.25\n",
            "jaxlib                        0.3.25+cuda11.cudnn805\n",
            "jieba                         0.42.1\n",
            "Jinja2                        2.11.3\n",
            "joblib                        1.2.0\n",
            "jpeg4py                       0.1.4\n",
            "jsonschema                    4.3.3\n",
            "jupyter-client                6.1.12\n",
            "jupyter-console               6.1.0\n",
            "jupyter-core                  5.1.0\n",
            "jupyterlab-widgets            3.0.3\n",
            "kaggle                        1.5.12\n",
            "kapre                         0.3.7\n",
            "keras                         2.9.0\n",
            "Keras-Preprocessing           1.1.2\n",
            "keras-vis                     0.4.1\n",
            "kiwisolver                    1.4.4\n",
            "korean-lunar-calendar         0.3.1\n",
            "langcodes                     3.3.0\n",
            "libclang                      14.0.6\n",
            "librosa                       0.8.1\n",
            "lightgbm                      2.2.3\n",
            "llvmlite                      0.39.1\n",
            "lmdb                          0.99\n",
            "locket                        1.0.0\n",
            "logical-unification           0.4.5\n",
            "LunarCalendar                 0.0.9\n",
            "lxml                          4.9.1\n",
            "Markdown                      3.4.1\n",
            "MarkupSafe                    2.0.1\n",
            "marshmallow                   3.19.0\n",
            "matplotlib                    3.2.2\n",
            "matplotlib-venn               0.11.7\n",
            "miniKanren                    1.0.3\n",
            "missingno                     0.5.1\n",
            "mistune                       0.8.4\n",
            "mizani                        0.7.3\n",
            "mkl                           2019.0\n",
            "mlxtend                       0.14.0\n",
            "more-itertools                9.0.0\n",
            "moviepy                       0.2.3.5\n",
            "mpmath                        1.2.1\n",
            "msgpack                       1.0.4\n",
            "multidict                     6.0.3\n",
            "multipledispatch              0.6.0\n",
            "multitasking                  0.0.11\n",
            "murmurhash                    1.0.9\n",
            "music21                       5.5.0\n",
            "natsort                       5.5.0\n",
            "nbconvert                     5.6.1\n",
            "nbformat                      5.7.0\n",
            "netCDF4                       1.6.2\n",
            "networkx                      2.8.8\n",
            "nibabel                       3.0.2\n",
            "nltk                          3.7\n",
            "notebook                      5.7.16\n",
            "numba                         0.56.4\n",
            "numexpr                       2.8.4\n",
            "numpy                         1.21.6\n",
            "oauth2client                  4.1.3\n",
            "oauthlib                      3.2.2\n",
            "okgrade                       0.4.3\n",
            "opencv-contrib-python         4.6.0.66\n",
            "opencv-python                 4.6.0.66\n",
            "opencv-python-headless        4.6.0.66\n",
            "openpyxl                      3.0.10\n",
            "opt-einsum                    3.3.0\n",
            "osqp                          0.6.2.post0\n",
            "packaging                     21.3\n",
            "palettable                    3.3.0\n",
            "pandas                        1.3.5\n",
            "pandas-datareader             0.9.0\n",
            "pandas-gbq                    0.17.9\n",
            "pandas-profiling              1.4.1\n",
            "pandocfilters                 1.5.0\n",
            "panel                         0.12.1\n",
            "param                         1.12.2\n",
            "parso                         0.8.3\n",
            "partd                         1.3.0\n",
            "pastel                        0.2.1\n",
            "pathlib                       1.0.1\n",
            "pathy                         0.10.0\n",
            "patsy                         0.5.3\n",
            "pep517                        0.13.0\n",
            "pexpect                       4.8.0\n",
            "pickleshare                   0.7.5\n",
            "Pillow                        7.1.2\n",
            "pip                           21.1.3\n",
            "pip-tools                     6.2.0\n",
            "platformdirs                  2.5.4\n",
            "plotly                        5.5.0\n",
            "plotnine                      0.8.0\n",
            "pluggy                        0.7.1\n",
            "pooch                         1.6.0\n",
            "portpicker                    1.3.9\n",
            "prefetch-generator            1.0.3\n",
            "preshed                       3.0.8\n",
            "prettytable                   3.5.0\n",
            "progressbar2                  3.38.0\n",
            "prometheus-client             0.15.0\n",
            "promise                       2.3\n",
            "prompt-toolkit                2.0.10\n",
            "prophet                       1.1.1\n",
            "proto-plus                    1.22.1\n",
            "protobuf                      3.19.6\n",
            "psutil                        5.4.8\n",
            "psycopg2                      2.9.5\n",
            "ptyprocess                    0.7.0\n",
            "py                            1.11.0\n",
            "pyarrow                       9.0.0\n",
            "pyasn1                        0.4.8\n",
            "pyasn1-modules                0.2.8\n",
            "pybullet                      3.2.5\n",
            "pycocotools                   2.0.6\n",
            "pycparser                     2.21\n",
            "pyct                          0.4.8\n",
            "pydantic                      1.10.2\n",
            "pydata-google-auth            1.4.0\n",
            "pydot                         1.3.0\n",
            "pydot-ng                      2.0.0\n",
            "pydotplus                     2.0.2\n",
            "PyDrive                       1.3.1\n",
            "pyemd                         0.5.1\n",
            "pyerfa                        2.0.0.1\n",
            "Pygments                      2.6.1\n",
            "pygobject                     3.26.1\n",
            "pylev                         1.4.0\n",
            "pymc                          4.1.4\n",
            "PyMeeus                       0.5.11\n",
            "pymongo                       4.3.3\n",
            "pymystem3                     0.2.0\n",
            "PyOpenGL                      3.1.6\n",
            "pyparsing                     3.0.9\n",
            "pyrsistent                    0.19.2\n",
            "pysimdjson                    3.2.0\n",
            "pysndfile                     1.3.8\n",
            "PySocks                       1.7.1\n",
            "pystan                        3.3.0\n",
            "pytest                        3.6.4\n",
            "python-apt                    0.0.0\n",
            "python-dateutil               2.8.2\n",
            "python-louvain                0.16\n",
            "python-slugify                7.0.0\n",
            "python-utils                  3.4.5\n",
            "pytz                          2022.6\n",
            "pyviz-comms                   2.2.1\n",
            "PyWavelets                    1.4.1\n",
            "PyYAML                        6.0\n",
            "pyzmq                         23.2.1\n",
            "qdldl                         0.1.5.post2\n",
            "qudida                        0.0.4\n",
            "regex                         2022.6.2\n",
            "requests                      2.23.0\n",
            "requests-oauthlib             1.3.1\n",
            "resampy                       0.4.2\n",
            "rpy2                          3.5.5\n",
            "rsa                           4.9\n",
            "scikit-image                  0.18.3\n",
            "scikit-learn                  1.0.2\n",
            "scipy                         1.7.3\n",
            "screen-resolution-extra       0.0.0\n",
            "scs                           3.2.2\n",
            "seaborn                       0.11.2\n",
            "Send2Trash                    1.8.0\n",
            "setuptools                    57.4.0\n",
            "setuptools-git                1.2\n",
            "Shapely                       1.8.5.post1\n",
            "six                           1.15.0\n",
            "sklearn-pandas                1.8.0\n",
            "smart-open                    5.2.1\n",
            "snowballstemmer               2.2.0\n",
            "sortedcontainers              2.4.0\n",
            "soundfile                     0.11.0\n",
            "spacy                         3.4.3\n",
            "spacy-legacy                  3.0.10\n",
            "spacy-loggers                 1.0.3\n",
            "Sphinx                        1.8.6\n",
            "sphinxcontrib-serializinghtml 1.1.5\n",
            "sphinxcontrib-websupport      1.2.4\n",
            "SQLAlchemy                    1.4.44\n",
            "sqlparse                      0.4.3\n",
            "srsly                         2.4.5\n",
            "statsmodels                   0.12.2\n",
            "sympy                         1.7.1\n",
            "tables                        3.7.0\n",
            "tabulate                      0.8.10\n",
            "tblib                         1.7.0\n",
            "tenacity                      8.1.0\n",
            "tensorboard                   2.9.1\n",
            "tensorboard-data-server       0.6.1\n",
            "tensorboard-plugin-wit        1.8.1\n",
            "tensorflow                    2.9.2\n",
            "tensorflow-datasets           4.6.0\n",
            "tensorflow-estimator          2.9.0\n",
            "tensorflow-gcs-config         2.9.1\n",
            "tensorflow-hub                0.12.0\n",
            "tensorflow-io-gcs-filesystem  0.28.0\n",
            "tensorflow-metadata           1.11.0\n",
            "tensorflow-probability        0.17.0\n",
            "termcolor                     2.1.1\n",
            "terminado                     0.13.3\n",
            "testpath                      0.6.0\n",
            "text-unidecode                1.3\n",
            "textblob                      0.15.3\n",
            "thinc                         8.1.5\n",
            "threadpoolctl                 3.1.0\n",
            "tifffile                      2022.10.10\n",
            "toml                          0.10.2\n",
            "tomli                         2.0.1\n",
            "toolz                         0.12.0\n",
            "torch                         1.13.0+cu116\n",
            "torchaudio                    0.13.0+cu116\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.14.0\n",
            "torchvision                   0.14.0+cu116\n",
            "tornado                       6.0.4\n",
            "tqdm                          4.64.1\n",
            "traitlets                     5.6.0\n",
            "tweepy                        3.10.0\n",
            "typeguard                     2.7.1\n",
            "typer                         0.7.0\n",
            "typing-extensions             4.4.0\n",
            "tzlocal                       1.5.1\n",
            "uritemplate                   3.0.1\n",
            "urllib3                       1.24.3\n",
            "vega-datasets                 0.9.0\n",
            "wasabi                        0.10.1\n",
            "wcwidth                       0.2.5\n",
            "webargs                       8.2.0\n",
            "webencodings                  0.5.1\n",
            "Werkzeug                      1.0.1\n",
            "wheel                         0.38.4\n",
            "widgetsnbextension            3.6.1\n",
            "wordcloud                     1.8.2.2\n",
            "wrapt                         1.14.1\n",
            "xarray                        0.20.2\n",
            "xarray-einstats               0.3.0\n",
            "xgboost                       0.90\n",
            "xkit                          0.0.0\n",
            "xlrd                          1.2.0\n",
            "xlwt                          1.3.0\n",
            "yarl                          1.8.2\n",
            "yellowbrick                   1.5\n",
            "zict                          2.2.0\n",
            "zipp                          3.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0mS0Xd9hRIk",
        "outputId": "807115aa-e79c-4cee-dde0-fb00de72beab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "import pybullet_envs\n",
        "from gym import make\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import Normal, MultivariateNormal\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BJfAjyIeaM6",
        "outputId": "d9755dff-fcc2-42e1-ead1-e169c42996e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to : cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aUR0Fy4g5Bq"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9wHwdfYyg7JE"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"Walker2DBulletEnv-v0\"\n",
        "\n",
        "LAMBDA = 0.95\n",
        "GAMMA = 0.99\n",
        "\n",
        "ACTOR_LR = 2e-5\n",
        "CRITIC_LR = 1e-5\n",
        "\n",
        "CLIP = 0.2\n",
        "ENTROPY_COEF = 1e-2\n",
        "BATCHES_PER_UPDATE = 2048\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "MIN_TRANSITIONS_PER_UPDATE = 2048\n",
        "MIN_EPISODES_PER_UPDATE = 4\n",
        "\n",
        "ITERATIONS = 1000\n",
        "\n",
        "def compute_lambda_returns_and_gae(trajectory):\n",
        "    lambda_returns = []\n",
        "    gae = []\n",
        "    last_lr = 0.\n",
        "    last_v = 0.\n",
        "    for _, _, r, _, v in reversed(trajectory):\n",
        "        ret = r + GAMMA * (last_v * (1 - LAMBDA) + last_lr * LAMBDA)\n",
        "        last_lr = ret\n",
        "        last_v = v\n",
        "        lambda_returns.append(last_lr)\n",
        "        gae.append(last_lr - v)\n",
        "\n",
        "    # Each transition contains state, action, old action probability, value estimation and advantage estimation\n",
        "    return [(s, a, p, v, adv) for (s, a, _, p, _), v, adv in zip(trajectory, reversed(lambda_returns), reversed(gae))]\n",
        "\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        # Advice: use same log_sigma for all states to improve stability\n",
        "        # You can do this by defining log_sigma as nn.Parameter(torch.zeros(...))\n",
        "        ''' YOUR CODE HERE '''\n",
        "        self.model =  nn.Sequential(\n",
        "              nn.Linear(state_dim, 256),\n",
        "              nn.ELU(),\n",
        "              nn.Linear(256, 256),\n",
        "              nn.ELU(),\n",
        "              nn.Linear(256, action_dim))\n",
        "        self.sigma = nn.Parameter(torch.ones(action_dim))\n",
        "\n",
        "    def compute_proba(self, state, action):\n",
        "        # Returns probability of action according to current policy and distribution of actions\n",
        "        ''' YOUR CODE HERE '''\n",
        "        mu = self.model(state)\n",
        "        dist = Normal(mu, torch.exp(self.sigma))\n",
        "        prob = torch.exp(dist.log_prob(action).sum(-1))\n",
        "        return prob, dist\n",
        "\n",
        "    def act(self, state):\n",
        "        # Returns an action (with tanh), not-transformed action (without tanh) and distribution of non-transformed actions\n",
        "        # Remember: agent is not deterministic, sample actions from distribution (e.g. Gaussian)\n",
        "        ''' YOUR CODE HERE '''\n",
        "        mu = self.model(state)\n",
        "        dist = Normal(mu, torch.exp(self.sigma))\n",
        "        action = dist.sample()\n",
        "\n",
        "        return torch.tanh(action), action, dist\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def get_value(self, state):\n",
        "        return self.model(state)\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.actor = Actor(state_dim, action_dim)\n",
        "        self.critic = Critic(state_dim)\n",
        "        self.actor_optim = Adam(self.actor.parameters(), ACTOR_LR)\n",
        "        self.critic_optim = Adam(self.critic.parameters(), CRITIC_LR)\n",
        "\n",
        "    def update(self, trajectories):\n",
        "        transitions = [t for traj in trajectories for t in traj] # Turn a list of trajectories into list of transitions\n",
        "        state, action, old_prob, target_value, advantage = zip(*transitions)\n",
        "        state = np.array(state)\n",
        "        action = np.array(action)\n",
        "        old_prob = np.array(old_prob)\n",
        "        target_value = np.array(target_value)\n",
        "        advantage = np.array(advantage)\n",
        "        advnatage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
        "\n",
        "\n",
        "        for _ in range(BATCHES_PER_UPDATE):\n",
        "            idx = np.random.randint(0, len(transitions), BATCH_SIZE) # Choose random batch\n",
        "\n",
        "            s = torch.tensor(state[idx]).float()\n",
        "            a = torch.tensor(action[idx]).float()\n",
        "            op = torch.tensor(old_prob[idx]).float() # Probability of the action in state s.t. old policy\n",
        "            v = torch.tensor(target_value[idx]).float() # Estimated by lambda-returns\n",
        "            adv = torch.tensor(advantage[idx]).float() # Estimated by generalized advantage estimation\n",
        "\n",
        "            ''' YOUR CODE HERE '''\n",
        "            # TODO: Update actor here\n",
        "            # calculate ratios\n",
        "            new_prob, dist = self.actor.compute_proba(s, a)\n",
        "            ratio = new_prob / op\n",
        "\n",
        "            # actor_loss\n",
        "            surr_loss = ratio * adv\n",
        "            clipped_surr_loss = (\n",
        "                torch.clamp(ratio, 1.0 - CLIP, 1.0 + CLIP) * adv\n",
        "            )\n",
        "\n",
        "            # entropy\n",
        "            entropy = dist.entropy().mean()\n",
        "\n",
        "            actor_loss = (\n",
        "                - torch.min(surr_loss, clipped_surr_loss).mean()\n",
        "                - entropy * ENTROPY_COEF\n",
        "            )\n",
        "\n",
        "            self.actor_optim.zero_grad()\n",
        "            actor_loss.backward(retain_graph=True)\n",
        "            self.actor_optim.step()\n",
        "\n",
        "\n",
        "            # TODO: Update critic here\n",
        "            # critic_loss\n",
        "            value = self.critic.get_value(s).flatten()\n",
        "            # critic_loss = (v - value).pow(2).mean()\n",
        "            critic_loss = F.smooth_l1_loss(value, v)\n",
        "\n",
        "            # train critic\n",
        "            self.critic_optim.zero_grad()\n",
        "            critic_loss.backward(retain_graph=True)\n",
        "            self.critic_optim.step()\n",
        "\n",
        "\n",
        "\n",
        "    def get_value(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(np.array([state])).float()\n",
        "            value = self.critic.get_value(state)\n",
        "        return value.cpu().item()\n",
        "\n",
        "    def act(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(np.array([state])).float()\n",
        "            action, pure_action, distr = self.actor.act(state)\n",
        "            prob = torch.exp(distr.log_prob(pure_action).sum(-1))\n",
        "        return action.cpu().numpy()[0], pure_action.cpu().numpy()[0], prob.cpu().item()\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.actor, \"./drive/MyDrive/Colab Notebooks/agent.pkl\")\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_policy(env, agent, episodes=5):\n",
        "    returns = []\n",
        "    for _ in range(episodes):\n",
        "        done = False\n",
        "        state = env.reset()\n",
        "        total_reward = 0.\n",
        "\n",
        "        while not done:\n",
        "            state, reward, done, _ = env.step(agent.act(state)[0])\n",
        "            total_reward += reward\n",
        "        returns.append(total_reward)\n",
        "    return returns\n",
        "\n",
        "\n",
        "def sample_episode(env, agent):\n",
        "    s = env.reset()\n",
        "    d = False\n",
        "    trajectory = []\n",
        "    while not d:\n",
        "        a, pa, p = agent.act(s)\n",
        "        v = agent.get_value(s)\n",
        "        ns, r, d, _ = env.step(a)\n",
        "        trajectory.append((s, pa, r, p, v))\n",
        "        s = ns\n",
        "    return compute_lambda_returns_and_gae(trajectory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELe0GF4uhKYD",
        "outputId": "0bc55b0d-ed3f-4a2f-85f5-e07cd18599d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 10, Reward mean: 19.090226625995417, Reward std: 8.417874725248836, Episodes: 1227, Steps: 20599\n",
            "Step: 20, Reward mean: 43.79200333704823, Reward std: 21.835163204675734, Episodes: 1522, Steps: 41408\n",
            "Step: 30, Reward mean: 55.80040976632737, Reward std: 9.578207709096326, Episodes: 1794, Steps: 62282\n",
            "Step: 40, Reward mean: 57.35467100295587, Reward std: 18.436448060534502, Episodes: 2056, Steps: 83256\n",
            "Step: 50, Reward mean: 63.82988005354157, Reward std: 3.2174581345465616, Episodes: 2303, Steps: 104105\n",
            "Step: 60, Reward mean: 66.60376613301747, Reward std: 3.5184390522395903, Episodes: 2553, Steps: 125027\n",
            "Step: 70, Reward mean: 66.89782017923889, Reward std: 14.048683497841706, Episodes: 2785, Steps: 145860\n",
            "Step: 80, Reward mean: 91.51735674849218, Reward std: 16.577037232981876, Episodes: 2998, Steps: 166763\n",
            "Step: 90, Reward mean: 66.24226068459888, Reward std: 22.962179238093164, Episodes: 3208, Steps: 187844\n",
            "Step: 100, Reward mean: 78.53412515200732, Reward std: 8.925402821401631, Episodes: 3411, Steps: 209088\n",
            "Step: 110, Reward mean: 119.48269380188408, Reward std: 102.2574849019404, Episodes: 3550, Steps: 230639\n",
            "Step: 120, Reward mean: 270.9725087841609, Reward std: 264.108955684713, Episodes: 3606, Steps: 254827\n",
            "Step: 130, Reward mean: 407.1377648987748, Reward std: 228.9583796498121, Episodes: 3653, Steps: 282248\n",
            "Step: 140, Reward mean: 248.56841969069515, Reward std: 219.7323817639082, Episodes: 3702, Steps: 307434\n",
            "Step: 150, Reward mean: 441.5322989603052, Reward std: 217.83903435556252, Episodes: 3750, Steps: 335085\n",
            "Step: 160, Reward mean: 493.57913640144244, Reward std: 183.69750329534102, Episodes: 3806, Steps: 359248\n",
            "Step: 170, Reward mean: 450.4747913013457, Reward std: 216.46472015804997, Episodes: 3858, Steps: 385253\n",
            "Step: 180, Reward mean: 171.0780917559552, Reward std: 213.96072465272468, Episodes: 3915, Steps: 411495\n",
            "Step: 190, Reward mean: 478.53144272978005, Reward std: 226.9987108056606, Episodes: 3958, Steps: 440335\n",
            "Step: 200, Reward mean: 522.902403011587, Reward std: 160.52622795916713, Episodes: 3999, Steps: 473555\n",
            "Step: 210, Reward mean: 503.1769381364367, Reward std: 199.9014564250664, Episodes: 4041, Steps: 508407\n",
            "Step: 220, Reward mean: 611.7086651702962, Reward std: 3.8334794954805678, Episodes: 4081, Steps: 543244\n",
            "Step: 230, Reward mean: 523.6973966838352, Reward std: 151.57863286608352, Episodes: 4121, Steps: 579158\n",
            "Step: 240, Reward mean: 489.95528641823694, Reward std: 229.95618324678816, Episodes: 4162, Steps: 608142\n",
            "Step: 250, Reward mean: 372.03231801316974, Reward std: 280.8772161930816, Episodes: 4203, Steps: 639160\n",
            "Step: 260, Reward mean: 498.2200608949468, Reward std: 163.95485432713605, Episodes: 4243, Steps: 666248\n",
            "Step: 270, Reward mean: 490.8607488281553, Reward std: 158.16863670512726, Episodes: 4294, Steps: 696787\n",
            "Step: 280, Reward mean: 600.9886129859058, Reward std: 4.56935242834776, Episodes: 4338, Steps: 723232\n",
            "Step: 290, Reward mean: 297.91816479348705, Reward std: 257.0025053506508, Episodes: 4381, Steps: 752809\n",
            "Step: 300, Reward mean: 404.91497889546946, Reward std: 230.0730235091516, Episodes: 4421, Steps: 784120\n",
            "Step: 310, Reward mean: 378.5566862834122, Reward std: 258.1468299952048, Episodes: 4468, Steps: 809256\n",
            "Step: 320, Reward mean: 604.12972198282, Reward std: 9.172995419656925, Episodes: 4509, Steps: 842230\n",
            "Step: 330, Reward mean: 603.1201351833312, Reward std: 4.818898180282315, Episodes: 4553, Steps: 871028\n",
            "Step: 340, Reward mean: 492.06245076260456, Reward std: 203.10633019867703, Episodes: 4593, Steps: 904483\n",
            "Step: 350, Reward mean: 601.1016327961788, Reward std: 5.094437220041898, Episodes: 4633, Steps: 938424\n",
            "Step: 360, Reward mean: 448.76716826666996, Reward std: 210.65400790131977, Episodes: 4676, Steps: 969029\n",
            "Step: 370, Reward mean: 494.84763463950765, Reward std: 218.90064349046213, Episodes: 4720, Steps: 1001821\n",
            "Step: 380, Reward mean: 433.4816335333136, Reward std: 201.8567048121605, Episodes: 4760, Steps: 1030032\n",
            "Step: 390, Reward mean: 295.57061232662693, Reward std: 250.8130029679418, Episodes: 4801, Steps: 1062373\n",
            "Step: 400, Reward mean: 592.8778448467472, Reward std: 4.799611430566556, Episodes: 4841, Steps: 1097547\n",
            "Step: 410, Reward mean: 582.1050046544585, Reward std: 7.6849854758731055, Episodes: 4883, Steps: 1129726\n",
            "Step: 420, Reward mean: 375.72349541864224, Reward std: 246.1986409693802, Episodes: 4926, Steps: 1160300\n",
            "Step: 430, Reward mean: 361.33835285224353, Reward std: 259.03322219862974, Episodes: 4968, Steps: 1187197\n",
            "Step: 440, Reward mean: 373.9490442463689, Reward std: 257.96909149880315, Episodes: 5012, Steps: 1215776\n",
            "Step: 450, Reward mean: 568.1714016850323, Reward std: 9.312660002002854, Episodes: 5053, Steps: 1249522\n",
            "Step: 460, Reward mean: 553.621120365087, Reward std: 1.9311987097758716, Episodes: 5094, Steps: 1282126\n",
            "Step: 470, Reward mean: 473.03545484546714, Reward std: 203.47279640661304, Episodes: 5135, Steps: 1313799\n",
            "Step: 480, Reward mean: 169.21116911346456, Reward std: 170.9683454184603, Episodes: 5182, Steps: 1340654\n",
            "Step: 490, Reward mean: 496.66745542558584, Reward std: 146.33205234814008, Episodes: 5232, Steps: 1366839\n",
            "Step: 500, Reward mean: 606.6587317247834, Reward std: 11.776183778657021, Episodes: 5275, Steps: 1393707\n",
            "Step: 510, Reward mean: 609.9646591424873, Reward std: 9.771327178410331, Episodes: 5320, Steps: 1426696\n",
            "Step: 520, Reward mean: 433.00121380583204, Reward std: 160.274165341835, Episodes: 5364, Steps: 1454333\n",
            "Step: 530, Reward mean: 361.32365665836176, Reward std: 197.72476652890896, Episodes: 5406, Steps: 1483007\n",
            "Step: 540, Reward mean: 197.30859936258088, Reward std: 235.33100283195833, Episodes: 5451, Steps: 1513098\n",
            "Step: 550, Reward mean: 352.5797595150272, Reward std: 184.4694681435315, Episodes: 5502, Steps: 1540105\n",
            "Step: 560, Reward mean: 397.1222269412169, Reward std: 200.81302422918964, Episodes: 5550, Steps: 1564757\n",
            "Step: 570, Reward mean: 296.60721579037425, Reward std: 210.1906293033752, Episodes: 5597, Steps: 1589390\n",
            "Step: 580, Reward mean: 321.4570822494903, Reward std: 289.5544097725178, Episodes: 5653, Steps: 1614720\n",
            "Step: 590, Reward mean: 515.9602968199627, Reward std: 208.24477522396506, Episodes: 5712, Steps: 1638966\n",
            "Step: 600, Reward mean: 309.24910584331576, Reward std: 216.84664744496672, Episodes: 5769, Steps: 1661545\n",
            "Step: 610, Reward mean: 298.9909355381091, Reward std: 205.7786360135852, Episodes: 5816, Steps: 1689804\n",
            "Step: 620, Reward mean: 430.21639972032307, Reward std: 374.3734545983908, Episodes: 5872, Steps: 1714267\n",
            "Step: 630, Reward mean: 304.5081399083057, Reward std: 173.07679411475726, Episodes: 5936, Steps: 1738606\n",
            "Step: 640, Reward mean: 398.3340741854414, Reward std: 205.1178761944076, Episodes: 6001, Steps: 1763027\n",
            "Step: 650, Reward mean: 542.5682555212181, Reward std: 322.6079290478566, Episodes: 6052, Steps: 1787698\n",
            "Step: 660, Reward mean: 568.5896663710716, Reward std: 278.9340405339124, Episodes: 6109, Steps: 1810526\n",
            "Step: 670, Reward mean: 331.4888725086321, Reward std: 334.9138943217435, Episodes: 6158, Steps: 1834138\n",
            "Step: 680, Reward mean: 537.9067506826971, Reward std: 380.27418889964673, Episodes: 6209, Steps: 1858512\n",
            "Step: 690, Reward mean: 838.2645605097083, Reward std: 346.14353279330777, Episodes: 6257, Steps: 1883213\n",
            "Step: 700, Reward mean: 403.2086952183219, Reward std: 211.49118327825335, Episodes: 6311, Steps: 1910571\n",
            "Step: 710, Reward mean: 587.2681520272745, Reward std: 332.47585793669754, Episodes: 6357, Steps: 1936073\n",
            "Step: 720, Reward mean: 778.0301916189094, Reward std: 370.0473089760982, Episodes: 6399, Steps: 1961890\n",
            "Step: 730, Reward mean: 876.6076088959268, Reward std: 432.80197787154276, Episodes: 6447, Steps: 1988301\n",
            "Step: 740, Reward mean: 592.2069043030517, Reward std: 396.5593439269033, Episodes: 6491, Steps: 2014707\n",
            "Step: 750, Reward mean: 542.5020742726954, Reward std: 410.8643670037513, Episodes: 6539, Steps: 2041523\n",
            "Step: 760, Reward mean: 786.2023421411483, Reward std: 457.0951549400829, Episodes: 6585, Steps: 2065666\n",
            "Step: 770, Reward mean: 948.2291115023199, Reward std: 240.47748851518008, Episodes: 6629, Steps: 2093096\n",
            "Step: 780, Reward mean: 545.1291305635353, Reward std: 451.77674517787676, Episodes: 6673, Steps: 2118962\n",
            "Step: 790, Reward mean: 526.6478899267858, Reward std: 442.2319361130841, Episodes: 6717, Steps: 2150030\n",
            "Step: 800, Reward mean: 869.4717033576301, Reward std: 357.28741734949176, Episodes: 6759, Steps: 2179319\n",
            "Step: 810, Reward mean: 1033.6554731632407, Reward std: 172.962796639571, Episodes: 6801, Steps: 2207556\n",
            "Step: 820, Reward mean: 1129.9472413964188, Reward std: 106.40567177831328, Episodes: 6843, Steps: 2234853\n",
            "Step: 830, Reward mean: 987.8318280573367, Reward std: 445.84863343413616, Episodes: 6887, Steps: 2263374\n",
            "Step: 840, Reward mean: 988.8045023141976, Reward std: 459.0589725895624, Episodes: 6930, Steps: 2290963\n",
            "Step: 850, Reward mean: 810.4020888093721, Reward std: 352.25303554776127, Episodes: 6977, Steps: 2321787\n",
            "Step: 860, Reward mean: 580.3304046404141, Reward std: 468.32224089422897, Episodes: 7017, Steps: 2350908\n",
            "Step: 870, Reward mean: 850.6692608305975, Reward std: 419.7452093904824, Episodes: 7063, Steps: 2379104\n",
            "Step: 880, Reward mean: 703.1142625005657, Reward std: 397.78795090417157, Episodes: 7106, Steps: 2408356\n",
            "Step: 890, Reward mean: 938.0661956094461, Reward std: 427.8819112636219, Episodes: 7149, Steps: 2435452\n",
            "Step: 900, Reward mean: 732.244891083771, Reward std: 433.18153325339875, Episodes: 7190, Steps: 2465202\n",
            "Step: 910, Reward mean: 877.6949812279192, Reward std: 414.5097965872016, Episodes: 7230, Steps: 2493801\n",
            "Step: 920, Reward mean: 789.9232005401009, Reward std: 330.18067132831817, Episodes: 7271, Steps: 2521532\n",
            "Step: 930, Reward mean: 602.9309803076487, Reward std: 328.5689974151434, Episodes: 7317, Steps: 2548565\n",
            "Step: 940, Reward mean: 893.6876588687168, Reward std: 420.05870124918357, Episodes: 7359, Steps: 2578970\n",
            "Step: 950, Reward mean: 225.94003566408455, Reward std: 240.85638494127483, Episodes: 7409, Steps: 2607426\n",
            "Step: 960, Reward mean: 741.9550082564675, Reward std: 397.4976304407158, Episodes: 7458, Steps: 2633420\n",
            "Step: 970, Reward mean: 352.43090747644885, Reward std: 177.7796633827867, Episodes: 7506, Steps: 2657799\n",
            "Step: 980, Reward mean: 905.7932266323767, Reward std: 364.48696362612844, Episodes: 7546, Steps: 2684909\n",
            "Step: 990, Reward mean: 746.4193687023478, Reward std: 426.51952350129545, Episodes: 7591, Steps: 2714045\n",
            "Step: 1000, Reward mean: 793.4111202108949, Reward std: 444.8899444449677, Episodes: 7632, Steps: 2744541\n"
          ]
        }
      ],
      "source": [
        "env = make(ENV_NAME)\n",
        "ppo = PPO(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
        "state = env.reset()\n",
        "episodes_sampled = 0\n",
        "steps_sampled = 0\n",
        "\n",
        "for i in range(ITERATIONS):\n",
        "    trajectories = []\n",
        "    steps_ctn = 0\n",
        "\n",
        "    while len(trajectories) < MIN_EPISODES_PER_UPDATE or steps_ctn < MIN_TRANSITIONS_PER_UPDATE:\n",
        "        traj = sample_episode(env, ppo)\n",
        "        steps_ctn += len(traj)\n",
        "        trajectories.append(traj)\n",
        "    episodes_sampled += len(trajectories)\n",
        "    steps_sampled += steps_ctn\n",
        "\n",
        "    ppo.update(trajectories)\n",
        "\n",
        "    if (i + 1) % (ITERATIONS//100) == 0:\n",
        "        rewards = evaluate_policy(env, ppo, 5)\n",
        "        print(f\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}, Episodes: {episodes_sampled}, Steps: {steps_sampled}\")\n",
        "        ppo.save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCO8eS7IxMfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hne6AOIohb1W"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pdqXy1lheAP"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.model = torch.load(__file__[:-8] + \"/agent.pkl\")\n",
        "\n",
        "    def act(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.tensor(np.array(state)).float()\n",
        "            ''' YOUR CODE HERE '''\n",
        "            actions = self.model(state)\n",
        "            return np.argmax(actions.cpu().numpy())\n",
        "\n",
        "    def reset(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dPC9M81n8EO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}